{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting notebook for the project\n",
    "# Imports\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Third-party imports # test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "\n",
    "# import cdo\n",
    "from cdo import *\n",
    "cdo = Cdo()\n",
    "\n",
    "# Local imports\n",
    "sys.path.append('/home/users/benhutch/skill-maps-historical')\n",
    "import dictionaries as dic\n",
    "import functions as fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters again\n",
    "# for the years 2-9 psl ULG (JJA)\n",
    "var = \"psl\"\n",
    "region = \"global\"\n",
    "region_grid = dic.gridspec_global\n",
    "forecast_range = \"2-9\"\n",
    "season = \"DJFM\" # weird season name for model\n",
    "observations_path = dic.obs\n",
    "obs_var_name = \"psl\"\n",
    "model_dict = dic.model_dictionary_psl_historical_badc\n",
    "start_year = 1960\n",
    "end_year = 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the observations first of all\n",
    "# obs = fnc.process_observations(var, region, region_grid,\n",
    "#                                forecast_range, season, observations_path,\n",
    "#                                obs_var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we want to merge the time axis and regrid the model (historical) data\n",
    "# # using the function call_mergetime_regrid\n",
    "# # this does not return anything, but saves the regridded data to a netcdf file\n",
    "# fnc.call_mergetime_regrid(model_dict, var, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing model:  BCC-CSM2-MR\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/BCC-CSM2-MR/regrid/psl_Amon_BCC-CSM2-MR_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  BCC-CSM2-MR :  3\n",
      "processing model:  MPI-ESM1-2-HR\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/MPI-ESM1-2-HR/regrid/psl_Amon_MPI-ESM1-2-HR_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  MPI-ESM1-2-HR :  1\n",
      "processing model:  CanESM5\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/CanESM5/regrid/psl_Amon_CanESM5_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  CanESM5 :  35\n",
      "processing model:  CMCC-CM2-SR5\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/CMCC-CM2-SR5/regrid/psl_Amon_CMCC-CM2-SR5_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  CMCC-CM2-SR5 :  1\n",
      "processing model:  HadGEM3-GC31-MM\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/HadGEM3-GC31-MM/regrid/psl_Amon_HadGEM3-GC31-MM_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  HadGEM3-GC31-MM :  4\n",
      "processing model:  EC-Earth3\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/EC-Earth3/regrid/psl_Amon_EC-Earth3_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  EC-Earth3 :  50\n",
      "processing model:  MPI-ESM1-2-LR\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/MPI-ESM1-2-LR/regrid/psl_Amon_MPI-ESM1-2-LR_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  MPI-ESM1-2-LR :  1\n",
      "processing model:  FGOALS-f3-L\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/FGOALS-f3-L/regrid/psl_Amon_FGOALS-f3-L_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  FGOALS-f3-L :  1\n",
      "processing model:  MIROC6\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/MIROC6/regrid/psl_Amon_MIROC6_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  MIROC6 :  50\n",
      "processing model:  IPSL-CM6A-LR\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/IPSL-CM6A-LR/regrid/psl_Amon_IPSL-CM6A-LR_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  IPSL-CM6A-LR :  9\n",
      "processing model:  NorCPM1\n",
      "type of var <class 'str'>\n",
      "type of model <class 'str'>\n",
      "type of region <class 'str'>\n",
      "regrid_files: /gws/nopw/j04/canari/users/benhutch/historical/psl/NorCPM1/regrid/psl_Amon_NorCPM1_historical_r*i?p?f?_*global_regrid.nc\n",
      "number of files for model  NorCPM1 :  1\n",
      "type of historical_data <class 'dict'>\n",
      "shape of historical_data ()\n"
     ]
    }
   ],
   "source": [
    "# Now we want to load the historical data\n",
    "# as a dictionary of xarray datasets for each model\n",
    "# using the load_historical_data function\n",
    "historical_data = fnc.load_historical_data(model_dict, var, region)\n",
    "\n",
    "# historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test = historical_data['NorCPM1'][0].psl\n",
    "\n",
    "# # test the processing functions individually\n",
    "# constrained_data = fnc.constrain_historical_data_season(historical_data, start_year=1960, end_year=2019\n",
    "#                                                         , season='DJFM', model='NorCPM1',member=0)\n",
    "\n",
    "# constrained_data\n",
    "\n",
    "# test the processing functions individually\n",
    "# test_BCC = historical_data['BCC-CSM2-MR']\n",
    "\n",
    "# # test the processing historical data function\n",
    "# processed_test_BCC = fnc.process_historical_data(historical_data, season='DJFM', forecast_range='2-9', \n",
    "#                                                 start_year=1960, end_year=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at the data we have processed\n",
    "# processed_test_BCC['BCC-CSM2-MR'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99577.47 ,  99577.47 ,  99577.47 , ...,  99577.47 ,  99577.47 ,\n",
       "         99577.47 ],\n",
       "       [ 98702.92 ,  98703.08 ,  98704.625, ...,  98713.29 ,  98707.836,\n",
       "         98704.46 ],\n",
       "       [ 98482.78 ,  98478.61 ,  98473.95 , ...,  98490.99 ,  98489.1  ,\n",
       "         98486.33 ],\n",
       "       ...,\n",
       "       [101010.57 , 101012.266, 101013.67 , ..., 101002.09 , 101005.56 ,\n",
       "        101008.375],\n",
       "       [100965.77 , 100966.87 , 100967.83 , ..., 100961.08 , 100962.91 ,\n",
       "        100964.46 ],\n",
       "       [100920.836, 100921.22 , 100921.516, ..., 100918.9  , 100919.67 ,\n",
       "        100920.305]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the ensemble mean\n",
    "import xarray as xr\n",
    "\n",
    "# Load the three datasets into a list\n",
    "datasets = [historical_data['BCC-CSM2-MR'][member] for member in historical_data['BCC-CSM2-MR']]\n",
    "\n",
    "# Combine the datasets into a single dataset along the 'member' dimension\n",
    "ensemble = xr.concat(datasets, dim='member')\n",
    "\n",
    "# Calculate the ensemble mean along the 'member' dimension\n",
    "ensemble_mean = ensemble.mean(dim='member')\n",
    "\n",
    "# Calculate the time mean along the 'time' dimension\n",
    "time_mean = ensemble_mean.mean(dim='time')\n",
    "\n",
    "time_mean.psl.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the parallel processing function\n",
    "processed_historical_data = fnc.process_historical_data_parallel(historical_data,\n",
    "                            season, forecast_range, start_year, end_year) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing model:  BCC-CSM2-MR\n",
      "processing member:  0\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  1\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  2\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing model:  MPI-ESM1-2-HR\n",
      "processing member:  0\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing model:  CanESM5\n",
      "processing member:  0\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  1\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  2\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  3\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  4\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  5\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  6\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  7\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  8\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  9\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  10\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  11\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  12\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  13\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  14\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  15\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  16\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n",
      "type of dic.season_timeshift:  <class 'list'>\n",
      "season:  -3\n",
      "forecast range:  2 - 9\n",
      "rolling mean value:  8\n",
      "processing member:  17\n",
      "type of data in constrain historical data season <class 'xarray.core.dataarray.DataArray'>\n",
      "member_data dimensions:  ('time', 'lat', 'lon')\n",
      "Calculating anomalies\n",
      "dimensions of data_climatology:  Frozen({'lon': 144, 'lat': 72})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now we want to call the function to process the historical data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# this will:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. constrain the data to the provided year range and season\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 4. calculate the running mean of these annual mean anomalies\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# then add the processed data back into the dictionary\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m processed_historical_data \u001b[38;5;241m=\u001b[39m \u001b[43mfnc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_historical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorical_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/skill-maps-historical/functions.py:774\u001b[0m, in \u001b[0;36mprocess_historical_data\u001b[0;34m(historical_data, season, forecast_range, start_year, end_year)\u001b[0m\n\u001b[1;32m    768\u001b[0m     print(\"Error, data is empty post anoms\")\n\u001b[1;32m    769\u001b[0m     return None\n\u001b[1;32m    771\u001b[0m # print the values of the data\n\u001b[1;32m    772\u001b[0m # print(\"constraints_data_anoms values: \", constrained_data_anoms.psl.values)\n\u001b[1;32m    773\u001b[0m \n\u001b[0;32m--> 774\u001b[0m # Calculate the annual mean anomalies\n\u001b[1;32m    775\u001b[0m constrained_data_anoms_annual = calculate_annual_mean_anomalies(constrained_data_anoms, season)\n\u001b[1;32m    777\u001b[0m # Check that the data is not empty\n",
      "File \u001b[0;32m~/skill-maps-historical/functions.py:592\u001b[0m, in \u001b[0;36mcalculate_annual_mean_anomalies\u001b[0;34m(constrained_data_anoms, season)\u001b[0m\n\u001b[1;32m    584\u001b[0m # Extract the data for this model and member\n\u001b[1;32m    585\u001b[0m member_data = constrained_data_anoms.psl\n\u001b[1;32m    587\u001b[0m # Verify that the data is an xarray dataset\n\u001b[1;32m    588\u001b[0m # if not isinstance(data, xr.Dataset):\n\u001b[1;32m    589\u001b[0m #     print(\"Error, data is not an xarray dataset\")\n\u001b[1;32m    590\u001b[0m #     return None\n\u001b[1;32m    591\u001b[0m \n\u001b[0;32m--> 592\u001b[0m # Check that the xarray dataset contains values other than NaN\n\u001b[1;32m    593\u001b[0m if member_data.isnull().all():\n\u001b[1;32m    594\u001b[0m     print(\"Error, data contains only NaN values\")\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/xarray/core/common.py:131\u001b[0m, in \u001b[0;36mAbstractArray.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/xarray/core/dataarray.py:642\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/xarray/core/variable.py:512\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/xarray/core/variable.py:252\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/array/core.py:1689\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1689\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m   1691\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/jaspy/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now we want to call the function to process the historical data\n",
    "# this will: test\n",
    "# 1. constrain the data to the provided year range and season\n",
    "# 2. calculate the climatology and remove this to create anomalies\n",
    "# 3. calculate the annual mean anomalies from the monthly mean anomalies\n",
    "# 4. calculate the running mean of these annual mean anomalies\n",
    "# then add the processed data back into the dictionary\n",
    "processed_historical_data = fnc.process_historical_data(historical_data, season, forecast_range, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the processed data\n",
    "processed_historical_data['NorCPM1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to process the historical data\n",
    "# in preperation for calculating the spatial correlations\n",
    "# this function constrains the years to only those available in all members\n",
    "# and then calculates the equally weighted ensemble mean of all members\n",
    "# using the function process_historical_data_spacial_correlations\n",
    "ensemble_mean = fnc.process_historical_data_spatial_correlations(processed_historical_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bens-python-environment",
   "language": "python",
   "name": "bens-python-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
